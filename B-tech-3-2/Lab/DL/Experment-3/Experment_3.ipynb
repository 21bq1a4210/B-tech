{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "o9JA08jvRWh2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Experment-3: A MLP using keras with TensorFlow for classification problem"
      ],
      "metadata": {
        "id": "GW3tRx16qCz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim : To implement a MLP using keras with TensorFlow for classification problem (heart disease predication).\n"
      ],
      "metadata": {
        "id": "jzyLL6VfqSMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Nessary Library and Module\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Setting Seaborn Style and Displaying TensorFlow Version\n",
        "sns.set()\n",
        "tf.__version__\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"/content/heart.csv\")\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data.drop('HeartDisease', axis=1)\n",
        "y = data['HeartDisease']\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "X = pd.get_dummies(X, columns=['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope'])\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Normalize the data using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model with dropout\n",
        "model = Sequential([\n",
        "    Dense(300, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(150, activation='relu'),\n",
        "    Dense(75, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert labels to 0 or 1 for binary classification\n",
        "y_train_binary = y_train.astype('float32')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_binary, epochs=10,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Displaying the summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Print layer information\n",
        "print(\"List of layers:\", model.layers)\n",
        "print(\"\\nName of the second layer:\", model.layers[1].name)\n",
        "\n",
        "# Accessing the second hidden layer and printing weights and bias\n",
        "hidden2 = model.layers[2]\n",
        "weights, bias = hidden2.get_weights()\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Bias:\", bias)\n",
        "\n",
        "# Plotting the training history\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "5qKNkXbuDIia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Explanation"
      ],
      "metadata": {
        "id": "o9JA08jvRWh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importing Nessary Library and Module\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "iSNWxdO5N02K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setting Seaborn Style and Displaying TensorFlow Version\n",
        "sns.set()\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "vIVL--wxE7S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the dataset\n",
        "data = pd.read_csv(\"/content/heart.csv\")\n",
        "data.info()"
      ],
      "metadata": {
        "id": "DxyhMvy_FB8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Separate features (X) and target variable (y)\n",
        "X = data.drop('HeartDisease', axis=1)\n",
        "y = data['HeartDisease']"
      ],
      "metadata": {
        "id": "oGldNwXuFZog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title One-hot encode categorical variables\n",
        "X = pd.get_dummies(X, columns=['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope'])\n",
        "X.head()"
      ],
      "metadata": {
        "id": "ETKPPJAoGAIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***pd.get_dummies():***\n",
        "* This function is provided by the Pandas library and is\n",
        "used for **one-hot encoding** categorical variables. It converts categorical variable(s) into dummy/indicator variables.\n",
        "* For each categorical column specified in the columns parameter, pd.get_dummies creates new binary columns (dummy variables) representing the unique values in those categorical columns.\n",
        "* The original categorical columns are then dropped from the DataFrame.\n",
        "* ***Example:***\n",
        "If the original DataFrame X had a column 'Sex' with values 'Male' and 'Female', after applying pd.get_dummies, it would create two new columns 'Sex_Male' and 'Sex_Female' with binary values (0 or 1) indicating the presence of each category."
      ],
      "metadata": {
        "id": "YQVYm71ZGVMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "bxPQfgZ4H48g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Normalize the data using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "-64GDwUOIM62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Normalizing the input data helps in achieving faster convergence during the training of neural networks. It prevents certain features with larger scales from dominating the learning process and ensures that the model is not sensitive to the scale of input features.\n",
        "* **Min-Max Scaling:** This is a specific type of normalization where the values are scaled to a specific range, typically between 0 and 1. Min-Max scaling is performed using the following formula:\n",
        "Xscaled = ( X - Xmin) / ( Xmax - Xmin )"
      ],
      "metadata": {
        "id": "Gfxg7dejIpIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build the neural network model with dropout\n",
        "model = Sequential([\n",
        "    Dense(300, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(150, activation='relu'),\n",
        "    Dense(75, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "lpYok5NnJlBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Sequential Model**: This line initializes a sequential model. In a sequential model, layers are added one by one in a linear stack.\n",
        "\n",
        "2. **Dense Layers**: The model consists of four dense layers:\n",
        "\n",
        "   - **First Layer (Input Layer)**:\n",
        "     - Dense(300, activation='relu', input_shape=(X_train.shape[1],)):\n",
        "       - 300 units in the layer.\n",
        "       - activation='relu': Rectified Linear Unit (ReLU) activation function is used.\n",
        "       - input_shape=(X_train.shape[1],): Specifies the shape of the input data. The input shape corresponds to the number of features in the training data.\n",
        "\n",
        "   - **Second Layer**:\n",
        "     - Dense(150, activation='relu'):\n",
        "       - 150 units.\n",
        "       - activation='relu': ReLU activation.\n",
        "\n",
        "   - **Third Layer**:\n",
        "     - Dense(75, activation='relu'):\n",
        "       - 75 units.\n",
        "       - activation='relu': ReLU activation.\n",
        "\n",
        "   - **Output Layer**:\n",
        "     - Dense(1, activation='sigmoid'):\n",
        "       - 1 unit, as it's a binary classification task (sigmoid activation is commonly used for binary classification).\n",
        "       - activation='sigmoid': Sigmoid activation function squashes the output between 0 and 1, making it suitable for binary classification.\n"
      ],
      "metadata": {
        "id": "K2db2okNKRkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fFR3AL72KZ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **`optimizer='adam'`**:\n",
        "   - The optimizer is the algorithm used to update the weights of the neural network during training. 'Adam' is a popular optimization algorithm that combines the advantages of two other methods: RMSprop and Momentum.\n",
        "   - `Adam` optimizes the learning rate during training.\n",
        "\n",
        "2. **`loss='binary_crossentropy'`**:\n",
        "   - The loss function (or objective function) is a measure of how well the model is performing. For binary classification problems, 'binary_crossentropy' is commonly used.\n",
        "   - Binary Crossentropy is suitable when dealing with binary classification tasks (two classes).\n",
        "\n",
        "- **Optimizer**: 'Adam' optimizer with adaptive learning rates.\n",
        "- **Loss Function**: 'Binary Crossentropy' suited for binary classification tasks.\n",
        "- **Metrics**: Monitoring the accuracy of the model during training and evaluation"
      ],
      "metadata": {
        "id": "2Cm5AeJOLdxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert labels to 0 or 1 for binary classification\n",
        "y_train_binary = y_train.astype('float32')"
      ],
      "metadata": {
        "id": "2_PjixyoMAlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**astype('float32')**: This method is used to change the data type of the target variable (y_train) to 'float32'. In binary classification, the target variable is often encoded as floats (0.0 and 1.0) instead of integers (0 and 1). This is done to ensure compatibility with the output layer's activation function, which is commonly set to 'sigmoid' for binary classification.\n",
        "\n",
        "* 0 → 0.0\n",
        "* 1 → 1.0"
      ],
      "metadata": {
        "id": "j3_iCWNFMdaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train the model\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_binary, epochs=10,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "a0FuC75iM1GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***history = model.fit(X_train_scaled, y_train_binary, epochs=10, verbose=0)*** : This line trains the model using the training data (X_train_scaled and y_train_binary). It specifies the number of training epochs (10) and sets verbose=0 to suppress training progress updates."
      ],
      "metadata": {
        "id": "ppKRDuNnM7X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Displaying the summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kthYHMJ2NDg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***model.summary()*** :This line prints a summary of the model's architecture, including the type of layers, the number of parameters, and the output shapes. The summary provides a concise overview of the neural network.\n"
      ],
      "metadata": {
        "id": "XYIJ4zWyNHV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Print layer information\n",
        "print(\"List of layers:\", model.layers)\n",
        "print(\"\\nName of the second layer:\", model.layers[1].name)"
      ],
      "metadata": {
        "id": "44wvNe_WNK90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Accessing the second hidden layer and printing weights and bias\n",
        "hidden2 = model.layers[2]\n",
        "weights, bias = hidden2.get_weights()\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Bias:\", bias)"
      ],
      "metadata": {
        "id": "8DGki7gvNZ4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting the training history\n",
        "pd.DataFrame(history.history).plot()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3ehghJrHNkVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate the model on the test set\n",
        "loss, acc = model.evaluate(X_test_scaled, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "311HgpIiNqB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}