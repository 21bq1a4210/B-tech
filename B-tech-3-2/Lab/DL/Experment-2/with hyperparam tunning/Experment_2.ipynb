{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Experment-1:A Multilayer Perceptron (MLP) using Keras with TensorFlow, and finetune neural network hyper parameters for regression problem"
      ],
      "metadata": {
        "id": "Hkvl9OuRh5iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim: To implement a Multilayer Perceptron (MLP) using Keras with TensorFlow, and finetune neural network hyper parameters for regression problem (house price prediction)."
      ],
      "metadata": {
        "id": "vQIfcDOviHmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q -U keras-tuner"
      ],
      "metadata": {
        "id": "1VsT7cHQT9CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAcE5WqLhrqu"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from kerastuner.tuners import Hyperband\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['price'] = data.target\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the model building function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units_1', 32, 256, step=32),\n",
        "                    activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "    model.add(Dense(units=hp.Int('units_2', 32, 128, step=32),\n",
        "                    activation='relu'))\n",
        "    model.add(Dense(units=hp.Int('units_3', 32, 64, step=32),\n",
        "                    activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Instantiate the Hyperband tuner\n",
        "tuner = Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=2,\n",
        "    factor=2,\n",
        "    directory='my_dir',\n",
        "    project_name='regression_tuner'\n",
        ")\n",
        "\n",
        "# Early stopping callback to monitor validation loss and stop training if no improvement after 3 epochs\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Search for the best hyperparameters and get the best model\n",
        "tuner.search(X_train_scaled, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=10, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Displaying the summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "mse = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f'Mean Squared Error on Test Set: {mse:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the code to to remove my_dir\n",
        "import shutil\n",
        "\n",
        "# Replace 'your_directory' with the actual path of the directory you want to remove\n",
        "directory_to_remove = 'my_dir'\n",
        "\n",
        "# Remove the directory recursively\n",
        "shutil.rmtree(directory_to_remove)\n",
        "\n",
        "print(f\"Directory '{directory_to_remove}' removed.\")\n"
      ],
      "metadata": {
        "id": "dqE0JHfxxuO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the above if you want to restart the hyperparm tunning. else it take already compiled best model"
      ],
      "metadata": {
        "id": "RT92pmDwlWUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code explanation"
      ],
      "metadata": {
        "id": "4HZxxCY8dNvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from kerastuner.tuners import Hyperband"
      ],
      "metadata": {
        "id": "noXfKKUjdQzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the California housing dataset\n",
        "# Load the California housing dataset using sklearn's fetch_california_housing function\n",
        "data = fetch_california_housing()\n",
        "\n",
        "# Create a DataFrame (df) using the dataset features as columns and add the target variable ('price')\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['price'] = data.target"
      ],
      "metadata": {
        "id": "9V-S_WNrdfxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fetch_california_housing function from sklearn**: It then creates a pandas DataFrame (df) using the dataset's feature values as columns and adds the target variable ('price') to the DataFrame. The resulting DataFrame contains both the input features and the target variable for further analysis or modeling."
      ],
      "metadata": {
        "id": "WhQqBufrexNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split the data into features and target\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X = df.drop('price', axis=1)  # X contains all columns except 'price'\n",
        "y = df['price']  # y contains only the 'price' column"
      ],
      "metadata": {
        "id": "gY28tYazdmJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **X**: This variable holds the features (independent variables) and is created by dropping the 'price' column using df.drop('price', axis=1). It includes all columns from the original DataFrame except for the 'price' column.\n",
        "\n",
        "* **y**: This variable represents the target variable (dependent variable) and is created by extracting the 'price' column from the original DataFrame using df['price']. It contains the values we want to predict or model."
      ],
      "metadata": {
        "id": "cgXR1BmzfUD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Vs-MqmIfdsN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, the dataset is split into training and testing sets using the `train_test_split` function from scikit-learn. Here's a breakdown of the parameters:\n",
        "\n",
        "- `X`: The feature dataset.\n",
        "- `y`: The target variable.\n",
        "- `test_size=0.2`: Specifies that 20% of the data will be used for testing, and the remaining 80% will be used for training.\n",
        "- `random_state=42`: Provides a seed for reproducibility, ensuring that the same split is obtained each time the code is executed.\n",
        "\n",
        "The result is four sets:\n",
        "\n",
        "- `X_train`: The training set features.\n",
        "- `X_test`: The testing set features.\n",
        "- `y_train`: The training set target variable.\n",
        "- `y_test`: The testing set target variable.\n",
        "\n",
        "This separation allows for training a model on one subset of the data (training set) and evaluating its performance on another subset (testing set), helping to assess the model's generalization to new, unseen data."
      ],
      "metadata": {
        "id": "U_NBGTTAftw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "NRrj6mT7dyfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, the input features (`X_train` and `X_test`) are standardized using the `StandardScaler` from scikit-learn. Standardization is a common preprocessing step in machine learning that transforms the data to have a mean of 0 and a standard deviation of 1. This is achieved by subtracting the mean and dividing by the standard deviation.\n",
        "\n",
        "Here's a breakdown of the code:\n",
        "\n",
        "- `scaler`: An instance of the `StandardScaler` class.\n",
        "- `fit_transform(X_train)`: Fits the scaler to the training data (`X_train`) and transforms it. The `fit_transform` method is used for training the scaler and applying the transformation simultaneously.\n",
        "- `transform(X_test)`: Applies the same transformation to the testing data (`X_test`). It's important to use the same parameters (mean and standard deviation) obtained from the training data to standardize the testing data.\n",
        "\n",
        "Standardization helps ensure that the features are on a similar scale, preventing one feature from dominating others during model training. This is especially important for algorithms sensitive to the scale of input features, such as gradient-based optimization algorithms used in neural networks."
      ],
      "metadata": {
        "id": "ik0ngnIogBmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the model building function\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Function to build a regression model with hyperparameters to be tuned.\n",
        "\n",
        "    Parameters:\n",
        "    - hp (HyperParameters): Hyperparameter tuning object.\n",
        "\n",
        "    Returns:\n",
        "    - model (Sequential): Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add the input layer with units sampled from the specified range\n",
        "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32),\n",
        "                    activation='relu', input_dim=X_train_scaled.shape[1]))\n",
        "\n",
        "    # Add the first hidden layer with units sampled from the specified range\n",
        "    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=128, step=32),\n",
        "                    activation='relu'))\n",
        "\n",
        "    # Add the second hidden layer with units sampled from the specified range\n",
        "    model.add(Dense(units=hp.Int('units_3', min_value=32, max_value=64, step=32),\n",
        "                    activation='relu'))\n",
        "\n",
        "    # Add the output layer with linear activation for regression\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Compile the model with Adam optimizer and mean squared error loss\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "6sRdo1CDd89N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `hp.Int`: This function is used to sample integers from a specified range. In this case, it's used to determine the number of units in each layer.\n",
        "- `Sequential`: The Keras sequential model is used to linearly stack layers.\n",
        "- `Dense`: This function adds a fully connected layer to the model.\n",
        "- `input_dim`: Specifies the input size for the first layer based on the number of features in the training data.\n",
        "- `activation='relu'`: Rectified Linear Unit (ReLU) activation is used for hidden layers.\n",
        "- `activation='linear'`: Linear activation is used for the output layer in regression problems.\n",
        "- `optimizer='adam'`: Adam optimizer is used for optimization.\n",
        "- `loss='mean_squared_error'`: Mean squared error is used as the loss function for regression.\n",
        "\n",
        "**Why should I have to use above function?**\n",
        "- The `build_model(hp)` function is designed for hyperparameter tuning using Keras Tuner. Keras Tuner is a library that helps you optimize hyperparameters for your machine learning models. In the context of the code you provided, here are the reasons for using `build_model(hp)`:\n",
        "\n",
        "1. **Hyperparameter Tuning**: The purpose of this function is to define the architecture of your neural network model, where certain architectural hyperparameters are dynamically set using the `hp` object from Keras Tuner. These hyperparameters include the number of units in each hidden layer.\n",
        "\n",
        "2. **Flexibility for Search Space**: By using `hp` functions such as `hp.Int`, you define a search space for hyperparameters. This allows Keras Tuner to search for the optimal combination of hyperparameters within the specified ranges during the tuning process.\n",
        "\n",
        "3. **Automated Search**: Keras Tuner performs an automated search over the hyperparameter space defined in `build_model(hp)` to find the combination that minimizes the specified objective (e.g., validation loss). This automates the process of finding the best hyperparameters, saving you from manually trying different combinations.\n",
        "\n",
        "4. **Reproducibility**: By encapsulating the model creation within a function, you ensure that each configuration is created in a consistent manner. This is crucial for reproducibility, especially when trying to reproduce experiments or share your work with others.\n",
        "\n",
        "5. **Integration with Keras Tuner**: The `hp` object provides a convenient way to sample hyperparameters, and it integrates seamlessly with Keras Tuner's search algorithms. It allows Keras Tuner to explore the hyperparameter space efficiently and identify the optimal configuration for your model.\n",
        "\n",
        "In summary, using `build_model(hp)` is a key step when leveraging Keras Tuner for hyperparameter tuning. It defines the search space and allows the tuner to explore different configurations to find the best set of hyperparameters for your regression model."
      ],
      "metadata": {
        "id": "kfXaFAdtgVeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instantiate the Hyperband tuner\n",
        "tuner = Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',  # Objective to minimize (validation loss in this case)\n",
        "    max_epochs=2,  # Maximum number of epochs for each trial\n",
        "    factor=2,  # Reduction factor for the number of trials in each bracket\n",
        "    directory='my_dir',  # Directory to store tuning results\n",
        "    project_name='regression_tuner'  # Name of the tuning project\n",
        ")\n"
      ],
      "metadata": {
        "id": "5mPwXuxBeCvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- **Hyperband Tuner**: The Hyperband tuner is used for hyperparameter tuning. It is a bandit-based optimization algorithm that efficiently explores the hyperparameter space by allocating resources to different configurations. Hyperband performs successive halving with early-stopping and aims to find the best set of hyperparameters for a given machine learning model.\n",
        "\n",
        "- **build_model**: The `build_model` function is a model-building function that defines the architecture of the neural network. It takes hyperparameters as input and returns a compiled Keras model.\n",
        "\n",
        "- **Objective ('val_loss')**: The objective of the tuning is to minimize the validation loss. During the tuning process, the tuner explores different sets of hyperparameters and evaluates their performance based on the validation loss.\n",
        "\n",
        "- **Max Epochs (2)**: The maximum number of epochs to train the model for each trial. This is a hyperparameter for the tuner, specifying the maximum duration of each trial.\n",
        "\n",
        "- **Factor (2)**: The reduction factor for the number of trials in each bracket. Hyperband uses successive halving, and the factor determines how many configurations are promoted to the next round. A higher factor explores more configurations but requires more resources.\n",
        "\n",
        "- **Directory ('my_dir')**: The directory where the tuning results are stored. It helps in resuming interrupted tuning processes or retrieving information about completed trials.\n",
        "\n",
        "- **Project Name ('regression_tuner')**: The name of the tuning project. It is used to organize and distinguish different tuning projects when using the same directory.\n",
        "\n",
        "In summary, the Hyperband tuner efficiently searches the hyperparameter space to find the best configuration for the neural network model, aiming to minimize the validation loss. The tuner automates the process of hyperparameter tuning, making it easier to find optimal settings for improved model performance."
      ],
      "metadata": {
        "id": "wh4c6oWyhmpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Early stopping callback to monitor validation loss and stop training if no improvement after 3 epochs\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',  # Metric to monitor for improvement\n",
        "    patience=3  # Number of epochs with no improvement after which training will be stopped\n",
        ")"
      ],
      "metadata": {
        "id": "i8f8w0KZeJgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `monitor='val_loss'`: The metric to monitor for improvement, which is the validation loss in this case.\n",
        "- `patience=3`: The number of epochs with no improvement after which training will be stopped. If the validation loss does not improve for 3 consecutive epochs, training will be halted early."
      ],
      "metadata": {
        "id": "jYQV8LJuh2of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Search for the best hyperparameters and get the best model\n",
        "tuner.search(\n",
        "    X_train_scaled,  # Scaled training features\n",
        "    y_train,  # Training target variable\n",
        "    epochs=5,  # Number of training epochs\n",
        "    validation_split=0.2  # Fraction of the training data to use as validation set\n",
        ")"
      ],
      "metadata": {
        "id": "muv1ABqeePMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `X_train_scaled`: The scaled training features used for training the model.\n",
        "- `y_train`: The training target variable.\n",
        "- `epochs=5`: The number of training epochs, i.e., the number of times the entire training dataset is passed forward and backward through the neural network.\n",
        "- `validation_split=0.2`: Fraction of the training data to use as a validation set. The model's performance on the validation set is monitored during training, and the hyperparameter search is guided by this validation performance."
      ],
      "metadata": {
        "id": "5d5n_axViFBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get the best hyperparameters, build the model, and train with early stopping\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(\n",
        "    X_train_scaled,  # Scaled training features\n",
        "    y_train,  # Training target variable\n",
        "    epochs=10,  # Number of training epochs\n",
        "    validation_split=0.2,  # Fraction of the training data to use as validation set\n",
        "    callbacks=[stop_early]  # Early stopping callback\n",
        ")"
      ],
      "metadata": {
        "id": "c5CS9gYxeSoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]`: Retrieves the best hyperparameters found during the search.\n",
        "- `model = tuner.hypermodel.build(best_hps)`: Builds a new model using the best hyperparameters.\n",
        "- `history = model.fit(...)`: Trains the model with the scaled training features (`X_train_scaled`) and training target variable (`y_train`). The training process is monitored using the validation set, and early stopping is applied with the specified callback (`stop_early`)."
      ],
      "metadata": {
        "id": "H8VDSK8XiSLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display the summary of the trained model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LrVCn2e4eato"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `model.summary()`: Displays a summary of the architecture and parameters of the trained neural network model. This includes information about each layer, the number of parameters, and the output shape at each layer. The summary provides a concise overview of the model's structure."
      ],
      "metadata": {
        "id": "p7e5yq2DioUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate the best model on the test set\n",
        "mse = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f'Mean Squared Error on Test Set: {mse:.4f}')"
      ],
      "metadata": {
        "id": "bSkNuj24ecxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "- `model.evaluate(X_test_scaled, y_test, verbose=0)`: Evaluates the trained model on the test set using the specified input features (`X_test_scaled`) and target values (`y_test`). The `verbose=0` parameter ensures that the evaluation is performed silently without displaying progress information.\n",
        "- `print(f'Mean Squared Error on Test Set: {mse:.4f}')`: Prints the mean squared error (MSE) on the test set. The `f-string` is used to format the output and display the MSE with four decimal places."
      ],
      "metadata": {
        "id": "VwDaPdtiiz14"
      }
    }
  ]
}